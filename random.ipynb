{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a \"genetic\" algorithm for chatbot personalization\n",
    "\n",
    "This is important for a number of reasons outlined in our paper *paper*. The way we achieve this is\n",
    "\n",
    "1. Collect responses and their internal representations according to some prompt\n",
    "2. Have the model rank responses and sample from this lineup (maybe with some randomness)\n",
    "3. Create control vectors and update the model\n",
    "4. Repeat this process with the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelforCausalLM, GenerationConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sarahpann/political-spectrum-questionnaire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: tokenizer(x, truncate=False, padding=False, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelforCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
    "                                             dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we move onto step 1. Let's write all of this information to a json file with serial numbers. The format should go something like:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"idx\": [0, 1, ..., n],\n",
    "    \"question\": [\"blah blah\", ..., \"blah blah\"],\n",
    "    \"response\": [\"blah blah\", ..., \"blah blah\"],\n",
    "    \"representation\": [[0.23812, ...], [0.239841, ...]]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = GenerationConfig.from_pretrained(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for example in dataset['right_dataset']:\n",
    "    out = model.generate(example['original_questions'], output_dictionary)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
